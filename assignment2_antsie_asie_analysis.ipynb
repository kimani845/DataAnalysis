{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772f4ca6",
   "metadata": {},
   "source": [
    "# ENEN90032 - Assignment 2\n",
    "\n",
    "## Hypothesis Test and Statistical Significance in Time Series Data\n",
    "\n",
    "This notebook implements Q1 (ANTSIE/ASIE anomaly analysis and significance tests) as requested. \n",
    "\n",
    "**Usage:** Downloaded ADS CSV files for Antarctic and Arctic monthly mean SIE are in the `data/` folder.<br>\n",
    "The Requirements are the same with the ones used in assignment 1.\n",
    "\n",
    "## What the Notebook does:\n",
    "    Loads both datasets from the `data/` folder.<br>\n",
    "    Computers the momthly anomalies using a 1979-2024 for reference. Seasonality is removed by the monthly climatology.<br>\n",
    "    Plots ensemble- style monthly anomaly figures (years overlaid; highlights the target year).<br>\n",
    "    Computes empirical probabilities:\n",
    "    Chance of observing ANTSIE in Aug 2023 or smaller using all months and using August-only samples.<br>\n",
    "    Same for ASIE Sep 2012.<br>\n",
    "    Repeats the ANTSIE calculation using Jan 2017â€“Dec 2022 as an alternative reference.<br>\n",
    "    Fits a linear trend to ASIE anomalies and tests slope significance using an AR(1)-corrected effective sample size (Santer-like correction).\n",
    "    If slope is significant, it recomputes anomaly/probability accounting for trend.<br>\n",
    "    All code is annotated and uses the allowed libraries (numpy, pandas, matplotlib, scipy.stats, math, statsmodels).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "090d49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "# Data files\n",
    "ANT_SIE_CSV = 'data/vishop_extent.csv'   \n",
    "ARC_SIE_CSV  = 'data/vishop_extent.csv'   \n",
    "# Reference periods\n",
    "REF_START = 1979\n",
    "REF_END = 2024\n",
    "\n",
    "# Alternative reference for Q1.3\n",
    "RECENT_START = 2017\n",
    "RECENT_END = 2022\n",
    "\n",
    "# Month/year of interest\n",
    "AUG_2023 = (2023, 8)\n",
    "SEP_2012 = (2012, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6aaba667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = (9,5)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40041da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ads_monthly_csv(path, date_col_candidates=None, value_col_candidates=None):\n",
    "    \"\"\"\n",
    "    Flexible loader for ADS monthly mean SIE CSVs.\n",
    "    Automatically skips metadata header lines until the first line starting with a digit.\n",
    "    Returns a DataFrame with columns ['date','year','month','sie'] where sie is numeric.\n",
    "    \"\"\"\n",
    "    # Detect start of data (skip metadata lines)\n",
    "    start_idx = 0\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line.strip() and line[0].isdigit():\n",
    "                start_idx = i\n",
    "                break\n",
    "\n",
    "    df = pd.read_csv(path, skiprows=start_idx)\n",
    "\n",
    "    # find date column\n",
    "    date_col = None\n",
    "    for c in df.columns:\n",
    "        if 'date' in c.lower():\n",
    "            date_col = c\n",
    "            break\n",
    "    if date_col is None:\n",
    "        # try constructing date from Year and Month columns\n",
    "        year_col = next((c for c in df.columns if 'year' in c.lower()), None)\n",
    "        month_col = next((c for c in df.columns if 'month' in c.lower()), None)\n",
    "        if year_col and month_col:\n",
    "            df['date'] = pd.to_datetime(df[year_col].astype(int).astype(str) + '-' + df[month_col].astype(int).astype(str) + '-01')\n",
    "        else:\n",
    "            raise RuntimeError('Could not find a date column or year/month columns in CSV')\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "\n",
    "    # find value column (sea ice extent)\n",
    "    value_col = None\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if any(k in lc for k in ['extent', 'sie', 'sea_ice', 'seaice']):\n",
    "            value_col = c\n",
    "            break\n",
    "    if value_col is None:\n",
    "        # pick first numeric column not the date\n",
    "        numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        numeric_cols = [c for c in numeric_cols if c not in [date_col]]\n",
    "        if len(numeric_cols) == 0:\n",
    "            raise RuntimeError('Could not find numeric SIE column')\n",
    "        value_col = numeric_cols[0]\n",
    "\n",
    "    out = df[['date', value_col]].rename(columns={value_col: 'sie'}).copy()\n",
    "    out = out.dropna(subset=['date'])\n",
    "    out['year'] = out['date'].dt.year\n",
    "    out['month'] = out['date'].dt.month\n",
    "    out = out.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    # Debug: show first few rows to confirm correct parsing\n",
    "    print(f\"[DEBUG] Loaded {len(out)} rows from {path}\")\n",
    "    print(out.head())\n",
    "\n",
    "    return out[['date','year','month','sie']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05a1abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_monthly_anomalies(df, ref_start=1979, ref_end=2024):\n",
    "    \"\"\"\n",
    "    Given a dataframe with year/month/sie columns, compute monthly climatology for the reference\n",
    "    period (ref_start--ref_end) and return a DataFrame with anomaly column 'sie_anom'.\n",
    "    Seasonality is removed by subtracting the long-term monthly mean for each calendar month.\n",
    "    \"\"\"\n",
    "    ref = df[(df['year'] >= ref_start) & (df['year'] <= ref_end)].copy()\n",
    "    # compute monthly climatology (mean for each calendar month)\n",
    "    clim = ref.groupby('month')['sie'].mean().rename('month_mean')\n",
    "    out = df.merge(clim, how='left', left_on='month', right_on='month')\n",
    "    out['sie_anom'] = out['sie'] - out['month_mean']\n",
    "    return out, clim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afaf75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensemble_anomalies(anom_df, highlight_year=2023, title='ANTSIE anomalies (1979-2024 reference)'):\n",
    "    \"\"\"\n",
    "    Plot monthly anomalies as an ensemble with years overlaid; highlight specified year.\n",
    "    \"\"\"\n",
    "    years = sorted(anom_df['year'].unique())\n",
    "    months = np.arange(1,13)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for y in years:\n",
    "        sub = anom_df[anom_df['year']==y]\n",
    "        if y == highlight_year:\n",
    "            plt.plot(sub['month'], sub['sie_anom'], label=str(y), linewidth=2.5, color='red')\n",
    "        else:\n",
    "            plt.plot(sub['month'], sub['sie_anom'], color='gray', alpha=0.5)\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('SIE anomaly (same units as input)')\n",
    "    plt.title(title)\n",
    "    plt.xlim(1,12)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc1f66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_prob_leq(value, series):\n",
    "    \"\"\"Empirical probability P(X <= value) using all samples in 'series'\"\"\"\n",
    "    series = np.asarray(series)\n",
    "    return np.mean(series <= value)\n",
    "\n",
    "# Helper: extract anomaly for a given year/month tuple\n",
    "def get_anom_for_month(anom_df, year_month_tuple):\n",
    "    y, m = year_month_tuple\n",
    "    row = anom_df[(anom_df['year']==y) & (anom_df['month']==m)]\n",
    "    if row.empty:\n",
    "        raise RuntimeError(f'No data for {year_month_tuple}')\n",
    "    return float(row['sie_anom'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8115329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ANTSIE...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['date', 'MonthlyMean']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the analysis (fill file paths in USER INPUT above before running)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoading ANTSIE...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ant = \u001b[43mload_ads_monthly_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mANT_SIE_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoaded ANTSIE rows:\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ant))\n\u001b[32m      6\u001b[39m ant_anom, ant_clim = calc_monthly_anomalies(ant, ref_start=REF_START, ref_end=REF_END)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mload_ads_monthly_csv\u001b[39m\u001b[34m(path, date_col_candidates, value_col_candidates)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mFlexible loader for ADS monthly mean SIE CSVs.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mReturns a DataFrame with columns ['date','year','month','sie'] where sie is numeric.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Use the comment parameter to skip metadata lines starting with #\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m#\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMonthlyMean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# find date column\u001b[39;00m\n\u001b[32m     12\u001b[39m date_col = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DataAnalysis/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DataAnalysis/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DataAnalysis/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DataAnalysis/venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DataAnalysis/venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.orig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.usecols_dtype == \u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols).issubset(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_names\n\u001b[32m    139\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.names) > \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DataAnalysis/venv/lib/python3.13/site-packages/pandas/io/parsers/base_parser.py:988\u001b[39m, in \u001b[36mParserBase._validate_usecols_names\u001b[39m\u001b[34m(self, usecols, names)\u001b[39m\n\u001b[32m    986\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    989\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    990\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m     )\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[31mValueError\u001b[39m: Usecols do not match columns, columns expected but not found: ['date', 'MonthlyMean']"
     ]
    }
   ],
   "source": [
    "# Run the analysis (fill file paths in USER INPUT above before running)\n",
    "print('Loading ANTSIE...')\n",
    "ant = load_ads_monthly_csv(ANT_SIE_CSV)\n",
    "print('Loaded ANTSIE rows:', len(ant))\n",
    "\n",
    "ant_anom, ant_clim = calc_monthly_anomalies(ant, ref_start=REF_START, ref_end=REF_END)\n",
    "\n",
    "# Plot ensemble and highlight 2023\n",
    "plot_ensemble_anomalies(ant_anom, highlight_year=2023, title='Antarctic SIE anomalies (1979-2024 ref)')\n",
    "\n",
    "# Q1.2: probability that we observe ANTSIE of Aug 2023 or smaller\n",
    "aug2023_anom = get_anom_for_month(ant_anom, AUG_2023)\n",
    "print('Aug 2023 anomaly (ANTSIE):', aug2023_anom)\n",
    "\n",
    "# (a) Using all observations in Jan 1979 - Dec 2024 (all months)\n",
    "all_series = ant_anom['sie_anom'].values\n",
    "p_all = empirical_prob_leq(aug2023_anom, all_series)\n",
    "print('P(ANOM <= Aug2023) using all months (1979-2024):', p_all)\n",
    "\n",
    "# (b) Using only August observations across years\n",
    "aug_series = ant_anom[ant_anom['month']==8]['sie_anom'].values\n",
    "p_aug = empirical_prob_leq(aug2023_anom, aug_series)\n",
    "print('P(ANOM <= Aug2023) using Augusts only:', p_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61564e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.3: Reference Jan 2017 - Dec 2022\n",
    "ant_anom_recent, _ = calc_monthly_anomalies(ant, ref_start=RECENT_START, ref_end=RECENT_END)\n",
    "aug2023_anom_recent = get_anom_for_month(ant_anom_recent, AUG_2023)\n",
    "print('Aug2023 anomaly relative to 2017-2022 ref:', aug2023_anom_recent)\n",
    "\n",
    "p_all_recent_ref = empirical_prob_leq(aug2023_anom_recent, ant_anom_recent['sie_anom'].values)\n",
    "aug_series_recent_ref = ant_anom_recent[ant_anom_recent['month']==8]['sie_anom'].values\n",
    "p_aug_recent_ref = empirical_prob_leq(aug2023_anom_recent, aug_series_recent_ref)\n",
    "print('P (all months, 2017-2022 ref):', p_all_recent_ref)\n",
    "print('P (August only, 2017-2022 ref):', p_aug_recent_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.4: Arctic ASIE and September 2012\n",
    "print('Loading ASIE...')\n",
    "arc = load_ads_monthly_csv(ARC_SIE_CSV)\n",
    "arc_anom, arc_clim = calc_monthly_anomalies(arc, ref_start=REF_START, ref_end=REF_END)\n",
    "plot_ensemble_anomalies(arc_anom, highlight_year=2012, title='Arctic SIE anomalies (1979-2024 ref)')\n",
    "\n",
    "sep2012_anom = get_anom_for_month(arc_anom, SEP_2012)\n",
    "print('Sep 2012 anomaly (ASIE):', sep2012_anom)\n",
    "\n",
    "p_all_arc = empirical_prob_leq(sep2012_anom, arc_anom['sie_anom'].values)\n",
    "sep_series = arc_anom[arc_anom['month']==9]['sie_anom'].values\n",
    "p_sep = empirical_prob_leq(sep2012_anom, sep_series)\n",
    "print('P(ASIE <= Sep2012) using all months:', p_all_arc)\n",
    "print('P(ASIE <= Sep2012) using Septembers only:', p_sep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.5: Fit linear trend to ASIE anomalies and test significance with AR(1) correction.\n",
    "# We'll follow a practical approach similar to Santer et al. (2000):\n",
    "#  - Fit OLS: anom = a + b * time\n",
    "#  - Estimate lag-1 autocorrelation (r1) of residuals\n",
    "#  - Compute effective sample size Ne = N*(1 - r1)/(1 + r1)\n",
    "#  - Adjust standard error of slope by factor sqrt(N/Ne)\n",
    "\n",
    "# Prepare data (use arc_anom, which covers 1979-2024)\n",
    "adf = arc_anom.copy().dropna(subset=['sie_anom']).reset_index(drop=True)\n",
    "adf = adf[(adf['year']>=REF_START) & (adf['year']<=REF_END)].copy()\n",
    "adf['time_index'] = np.arange(len(adf))  # time in months\n",
    "X = sm.add_constant(adf['time_index'])\n",
    "y = adf['sie_anom'].values\n",
    "model = sm.OLS(y, X).fit()\n",
    "slope = model.params[1]\n",
    "se_slope = model.bse[1]\n",
    "print('OLS slope (per month):', slope, 'SE:', se_slope)\n",
    "\n",
    "# residual lag-1 autocorrelation\n",
    "resid = model.resid\n",
    "# compute lag-1\n",
    "r1 = np.corrcoef(resid[:-1], resid[1:])[0,1]\n",
    "N = len(resid)\n",
    "Ne = N * (1 - r1) / (1 + r1) if (1 + r1) != 0 else N\n",
    "print('Lag-1 autocorr of residuals r1 =', r1)\n",
    "print('Approx effective sample size Ne =', Ne)\n",
    "\n",
    "# adjust standard error\n",
    "se_adj = se_slope * math.sqrt(N / Ne)\n",
    "t_adj = slope / se_adj\n",
    "# approximate df\n",
    "df_eff = max(int(np.floor(Ne - 2)), 1)\n",
    "p_adj = 2 * (1 - stats.t.cdf(abs(t_adj), df=df_eff))\n",
    "print('Adjusted t-statistic for slope:', t_adj, 'p-value (two-sided):', p_adj)\n",
    "\n",
    "# If slope significant, recompute anomalies removing trend (monthly seasonal already removed)\n",
    "if p_adj < 0.05:\n",
    "    print('Slope is significant at 5% -> include trend when computing anomalies for ASIE.')\n",
    "else:\n",
    "    print('No significant slope at 5% -> trend inclusion optional.')\n",
    "\n",
    "# If trend is significant, compute anomaly relative to trend+seasonal expected value for Sep2012\n",
    "if p_adj < 0.05:\n",
    "    # build trend line and subtract\n",
    "    trend = model.predict(X)\n",
    "    adf['sie_anom_trend_removed'] = adf['sie_anom'] - trend\n",
    "    # find index for Sep2012\n",
    "    val_row = adf[(adf['year']==2012) & (adf['month']==9)]\n",
    "    if not val_row.empty:\n",
    "        sep2012_trend_anom = float(val_row['sie_anom_trend_removed'].iloc[0])\n",
    "        print('Sep2012 anomaly after removing trend (and seasonal):', sep2012_trend_anom)\n",
    "        p_sep_trend = empirical_prob_leq(sep2012_trend_anom, adf['sie_anom_trend_removed'].values)\n",
    "        print('Probability (Sep2012 <= value) after trend removal:', p_sep_trend)\n",
    "    else:\n",
    "        print('Sep2012 not found in trend dataset.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
